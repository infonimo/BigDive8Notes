# MongoDB
*Alessandro Molina, AXANT*

### MongoDB Dataworkflow

there's a cost in using relational DB
table tuple with a key with fixed known type

classic relational db are row oriented

column oriented relational db (vertical)
for each column -> all values rows

metric operational are optimised to run in all db

at the beginning we don't know what kind of data we are going to store

### SchemaLess

key value db system: KEY -> VALUE
regGis?
document based db system: KEY->DOCUMENTS (containing multiple (name,type,value))
for example JSON
a document can contain another document

**MONGO**
- is document based
- support indexing
- support autosharding -> for splitting data around nodes (difficult for relational db)
has aggregation pipeline
- builtin support for mapreduce -> analysis on distributed nodes, on data scattered.
- scalable and optimized for bigdata
- free format, works with uneven shaped data or incomplete

computation engine
  you can perform analysis thought map-reduce
  and with aggregation pipeline
  you need analyze only a subset of the data at time

V8 (written in c) adds to Mongo computation engine the data storage

The speed layer of dataingestion (es: IOT)
distribute the workload in all the nodes

### SHARDING
python application
  using pymongo as the driver
  request a query to all the query routers
    (you can have one query router for each application)
    the query router doesn't contain data but the data configuration informations
    the query routers know what the other routers contain, the data distribution,
    so they send only the needed data and avoid repetition
    the configuration it's managed by the config server

a shard it's a piece of the data
the shard it's replicated in multiple nodes (Primary)

if a node fails the query router knows how to fulfil the request

### MAPREDUCE
two steps:
mapping step: the input is divided in subparts until it reaches its minimum
  processable particle, it get processed and the corresponding result gets returned

a map for each node

reduction step: each value processed in mapping step, the reduce nodes join the
 values together to a single result and send it to compile output node
 (the final reduce node)

i can distribute the reduce phases with replication and creating
  an hierarchy, a pyramid


### EXERCISES and mongo setup
mongo

sudo mongod

control c can kill safely mongo and mongod


https://docs.mongodb.com
brew services start mongodb-community@4.0
brew services stop mongodb-community


https://github.com/mongodb/homebrew-brew

mongod --config /usr/local/etc/mongod.conf

mongo admin --eval "db.shutdownServer()"

~~~~~
HARD KILL
pgrep mongo
kill 'numprocess'
launchctl list | grep mongo
~~~~~

```
sudo mongod
```
the configuration file (/usr/local/etc/mongod.conf)
the log directory path (/usr/local/var/log/mongodb)
the data directory path (/usr/local/var/mongodb)


process stuff:
top
kill -9 pid
ps -a


### collection data

ObjectId it's generated by Mongo  = timsetamp machineid processid counter

modify a document happens in a Transition
FindAndModify, update -> atomic transations per document
multiversion concurrency control mvcc

woredtoger data is stored in pages managed as a tree
cacheSizeGB tells how may pages are kept in memory
pages are compressed and get deflated when loaded from the disk

set cacheSizeGB is 60% RAM
  at least 2 GB of free memory
  and at least 4 GB of ram

mongo create checkpoints each 60 seconds
journal is used to ensure uncorrupted data between checkpoints

Address already in use means it's already runnings

it you update_one and the search find more results, a random one will be updated

### SubDocuments

updating with $push you can have duplicate in a list for ex, solution: $addToSet

cover the query: some times you don't need to fetch the data

### Indexes

every single node have an index

b-tree indexes

the create index number argument if is -1 it starts from the end creating the b-tree, it can useful to set when we need ordering

### covered Queries

> `db.users.find({score:{'$lt'} ... }, { score:1, \_id:0})`
>                               
> the second argoment it's the projection (for \` symbol alt + \\ )

if a query projection contains only indexed fields, you won't need to scan the collection

multiple kind of index types:
- compound for multiple fields can be represented as a cube
- multikey index contain a list of SubDocuments related to the key
- **geospatial** indexing

  if you create a classification distance in a suggestion system you can use this, it works with any distance on a cartesian space
- texts only one for collection, it can be used in a compound index but only if the other it' unique for each text?

Index Proprieties:
- ttl
- unique
- sparse



db.blog.getIndexes()

\_id index it's created by default

if you order by timestamp you can order by creation date because the first part it's creation timestamp

find something 'similar to'
es:
  size, color


  COLLSCAN : I had to check all of them

  limit(1) to get at maximum one value

### Aggregation pipeline

commands executed in an order, aggregate
commands:
+ {$unwind: '$tags'}
+ $limit,
+ $project
+ $group
+ $sort
+ $match


there is a command mongo-import to import json and csv files or tsv


df = DataFrame(list( --find query --)) it's not really efficient

BSON-numpy it's a project in prototype

thereÃ¬s mongoexport

open an event

### MapReduce
map -> reduce

mapper and reduce steps

this it's the current mongo db document
emits one each time
`````
tweets.map_reduce(
  map='''function(){
      var tags = this.entities.hashtags;
      for(var 1=0; i<tags.lenght; i++)
        emit(tags[i].text, 1)
  }
  ''',
  reduce='''function(key, values ){
    return Array.sum(values);
  }''',
  'out="name")
`````

  after you can call it from the collection

  with a lot of nodes use MAPREDUCE



sse.py ?
the watcher it's an iterator
